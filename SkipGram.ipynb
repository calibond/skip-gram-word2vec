{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kW9zL4V6KDVc"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECD5r2hFKDVd"
   },
   "source": [
    "# Skip Gram Model\n",
    "\n",
    "\n",
    " - Python's Gensim module: https://radimrehurek.com/gensim/ (install using pip)\n",
    "\n",
    "    Note: The most important hyper parameters of skip-gram/CBOW are vector size and windows size_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vsocwry-KDVe",
    "outputId": "4b0716cb-eeed-4e6f-a112-601dd872b47d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TIcyAqlk6Qpy",
    "outputId": "95c9dd06-38d9-4f25-d318-857872688bb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load('word2vec-google-news-300') # this step might take ~10-15 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZF74G9bDKDVh"
   },
   "source": [
    "Finding the cosine similarity between the following word pairs\n",
    "- (President, Election)\n",
    "- (Hot, Warm)\n",
    "- (England, London)\n",
    "- (France, ball)\n",
    "- (small, smaller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SZD5ZaMvKDVk"
   },
   "outputs": [],
   "source": [
    "#Replace 0 with the code / value; Do not delete this cell\n",
    "from scipy.spatial.distance import cosine\n",
    "word_pairs = [('President', 'Election'), ('Hot', 'Warm'), ('England', 'London'),\n",
    "              ('France', 'ball'), ('small', 'smaller')]\n",
    "similarity_pair1 = 1 - cosine(model[word_pairs[0][0]], model[word_pairs[0][1]]) if word_pairs[0][0] in model and word_pairs[0][1] in model else 0\n",
    "similarity_pair2 = 1 - cosine(model[word_pairs[1][0]], model[word_pairs[1][1]]) if word_pairs[1][0] in model and word_pairs[1][1] in model else 0\n",
    "similarity_pair3 = 1 - cosine(model[word_pairs[2][0]], model[word_pairs[2][1]]) if word_pairs[2][0] in model and word_pairs[2][1] in model else 0\n",
    "similarity_pair4 = 1 - cosine(model[word_pairs[3][0]], model[word_pairs[3][1]]) if word_pairs[3][0] in model and word_pairs[3][1] in model else 0\n",
    "similarity_pair5 = 1 - cosine(model[word_pairs[4][0]], model[word_pairs[4][1]]) if word_pairs[4][0] in model and word_pairs[4][1] in model else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcqpWCjJKDVs"
   },
   "source": [
    "Writing expressions to extract the vector representations of the words:\n",
    "\n",
    "- France\n",
    "- England\n",
    "- smaller\n",
    "- bigger\n",
    "- rocket\n",
    "- big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ElgK5QLuGi6S"
   },
   "outputs": [],
   "source": [
    "vector_1 = model['France'][:5] if 'France' in model else 0\n",
    "vector_2 = model['England'][:5] if 'England' in model else 0\n",
    "vector_3 = model['smaller'][:5] if 'smaller' in model else 0\n",
    "vector_4 = model['bigger'][:5] if 'bigger' in model else 0\n",
    "vector_5 = model['rocket'][:5] if 'rocket' in model else 0\n",
    "vector_6 = model['big'][:5] if 'big' in model else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UBnMwiXKDVy"
   },
   "source": [
    "Finding the euclidean distances between the word pairs :\n",
    "\n",
    "- (France, England)\n",
    "- (smaller, bigger)\n",
    "- (England, London)\n",
    "- (France, Rocket)\n",
    "- (big, bigger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JYUdXmCOMg45"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "eu_dist1 = np.linalg.norm(model['France'] - model['England']) if 'France' in model and 'England' in model else 0\n",
    "eu_dist2 = np.linalg.norm(model['smaller'] - model['bigger']) if 'smaller' in model and 'bigger' in model else 0\n",
    "eu_dist3 = np.linalg.norm(model['England'] - model['London']) if 'England' in model and 'London' in model else 0\n",
    "eu_dist4 = np.linalg.norm(model['France'] - model['Rocket']) if 'France' in model and 'England' in model else 0\n",
    "eu_dist5 = np.linalg.norm(model['big'] - model['bigger']) if 'big' in model and 'bigger' in model else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HsSg0l2UKDV6",
    "outputId": "dfdd361f-e724-484b-b629-e627c8723f74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0151067\n",
      "1.8618743\n",
      "2.8752837\n",
      "3.892071\n",
      "1.9586496\n"
     ]
    }
   ],
   "source": [
    "print(eu_dist1)\n",
    "print(eu_dist2)\n",
    "print(eu_dist3)\n",
    "print(eu_dist4)\n",
    "print(eu_dist5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvO2iU7QKDWA"
   },
   "source": [
    "Using Word2Vec to find the 2 closest words:\n",
    "- (King - Man + Queen)\n",
    "- (bigger - big + small)\n",
    "- (waiting - wait + run)\n",
    "- (Texas + Milwaukee â€“ Wisconsin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jCxWmA1eKDWB"
   },
   "outputs": [],
   "source": [
    "closest1 = model.most_similar(positive=['King', 'Queen'], negative=['Man'], topn=2)\n",
    "closest2 = model.most_similar(positive=['bigger', 'small'], negative=['big'], topn=2)\n",
    "closest3 = model.most_similar(positive=['waiting', 'run'], negative=['wait'], topn=2)\n",
    "closest4 = model.most_similar(positive=['Texas', 'Wisconsin'], negative=['Milwaukee'], topn=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "io9elfD8KDWE",
    "outputId": "812b0aa1-cadf-4a5b-b70d-c140d6cf68ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Queen_Elizabeth', 0.5257916450500488), ('monarch', 0.5004087090492249)]\n",
      "[('larger', 0.7402471303939819), ('smaller', 0.7329993844032288)]\n",
      "[('running', 0.5654535889625549), ('runs', 0.49639999866485596)]\n",
      "[('Nebraska', 0.6184834241867065), ('Arkansas', 0.5827385783195496)]\n"
     ]
    }
   ],
   "source": [
    "print(closest1)\n",
    "print(closest2)\n",
    "print(closest3)\n",
    "print(closest4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erUu4u71KDWJ"
   },
   "source": [
    "***Using Google News dataset to apply K-means clustering to find most representative words***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "iqTEPYr_YuRu"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import random\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "words = list(model.index_to_key)\n",
    "sample1 = random.sample(words, 25000)\n",
    "vectors = np.array([model[i] for i in sample1])\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans.fit(vectors)\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "most_rep_cluster1 = model.similar_by_vector(cluster_centers[0], topn=5)\n",
    "most_rep_cluster2 = model.similar_by_vector(cluster_centers[1], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M3jN02fOKDWK",
    "outputId": "36791bd7-6c22-4ac8-9a95-8a1bd0f4ac63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('http_dol##.net_index###.html_http', 0.9178717732429504), ('dol##.net_index####.html_http_dol##.net', 0.907823920249939), ('index###.html_http_dol##.net_index###.html', 0.906944751739502), ('Deltagen_undertakes', 0.9038156270980835), ('By_TRICIA_SCRUGGS', 0.9010686874389648)]\n",
      "[('Emil_Protalinski_Published', 0.9201086163520813), ('By_HuDie_####-##-##', 0.9168179035186768), ('By_QianMian_####-##-##', 0.9161686301231384), ('BY_GEOFF_KOHL', 0.914146363735199), ('By_XiaoBing_####-##-##', 0.9127659797668457)]\n"
     ]
    }
   ],
   "source": [
    "print(most_rep_cluster1)\n",
    "print(most_rep_cluster2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06H4qp0FJOyI"
   },
   "source": [
    "Categorical cross entropy as a primary loss function for the skip gram model. The skip-gram's goal is to predict context words for a target word. Categorical cross entropy works with a softmax activation function which calculates the probability distirbution over all of the words. This function minimizes the difference between the model's predictions and the actual words, making it effective for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbpuJx9CKDWV"
   },
   "source": [
    "Finding at least 2 interesting word vec combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pQM8C_T7KDWW",
    "outputId": "c8bb1608-12d2-4c78-a2c8-807e8ef004fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Milan', 0.7222141623497009), ('Rome', 0.702830970287323)]\n",
      "[('teachers', 0.5810958743095398), ('PE_teacher', 0.556725800037384)]\n",
      "[('teachers', 0.6448071002960205), ('guidance_counselor', 0.6279474496841431)]\n",
      "[('shirt', 0.6057167053222656), ('blazer', 0.5627408027648926)]\n"
     ]
    }
   ],
   "source": [
    "result1 = model.most_similar(positive=['Paris', \"Italy\"], negative=['France'], topn=2)\n",
    "result2 = model.most_similar(positive=['teacher', 'man'], negative=['woman'], topn=2)\n",
    "result3 = model.most_similar(positive=['teacher', 'woman'], negative=['man'], topn=2)\n",
    "result4 = model.most_similar(positive=['summer', 'sweater'], negative=['winter'], topn=2)\n",
    "\n",
    "print(result1)\n",
    "print(result2)\n",
    "print(result3)\n",
    "print(result4)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
